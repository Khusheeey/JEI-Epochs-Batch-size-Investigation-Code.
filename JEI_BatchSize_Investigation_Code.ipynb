{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khusheeey/JEI-Epochs-Batch-size-Investigation-Code./blob/main/JEI_BatchSize_Investigation_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wOdeUbaFQc8"
      },
      "outputs": [],
      "source": [
        "# BATCH & TRIAL NUMBER\n",
        "batch_size=100\n",
        "trial_num=30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUqfL2k5vEqq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "tf.config.list_physical_devices('GPU')\n",
        "\n",
        "# Mount drive and change directory to folder location containing the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/My Drive/JEI pulsar train data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTOabe7hvSP7",
        "outputId": "123a7577-96e4-4be4-dba8-85ee9c93b4fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 120 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# create data pipeline\n",
        "data = tf.keras.utils.image_dataset_from_directory('/content/drive/My Drive/JEI pulsar train data',batch_size=batch_size)\n",
        "data_iterator = data.as_numpy_iterator()\n",
        "batch = data_iterator.next()\n",
        "\n",
        "# 1 = PULSAR, 0 = NOT PULSAR\n",
        "# viewing 4 images from a batch\n",
        "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
        "for idx, img in enumerate(batch[0][:4]):\n",
        "    ax[idx].imshow(img.astype(int))\n",
        "    ax[idx].title.set_text(batch[1][idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7AwEZ-BvVi7"
      },
      "outputs": [],
      "source": [
        "# dividing by 255 to make values btw 0-1\n",
        "\n",
        "data = data.map(lambda x,y: (x/255, y))\n",
        "scaled_iterator = data.as_numpy_iterator()\n",
        "batch = scaled_iterator.next()\n",
        "\n",
        "# viewing 4 images from a batch\n",
        "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
        "for idx, img in enumerate(batch[0][:4]):\n",
        "    ax[idx].imshow(img)\n",
        "    ax[idx].title.set_text(batch[1][idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaZCky_EvXZi"
      },
      "outputs": [],
      "source": [
        "# splitting data into training and validation sets (100 of the 120 for training, and 20 of the 120 for testing)\n",
        "\n",
        "train_size = int(len(data)*(100/120))\n",
        "val_size = int(len(data)*(20/120))\n",
        "\n",
        "train = data.take(train_size)\n",
        "val = data.skip(train_size).take(val_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHt8o6AGvdh1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Building the model\n",
        "model.add(Conv2D(16, (3,3), 1, activation='relu', input_shape=(256,256,3)))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile('adam', loss=tf.losses.BinaryCrossentropy(), metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_M3m4fv4zX0"
      },
      "outputs": [],
      "source": [
        "# initiating training\n",
        "hist = model.fit(train, epochs=5, validation_data=val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeaiN571SlTe"
      },
      "source": [
        "**TESTING SECTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS5bPW2R8Jln",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# changing to directory with test dataset\n",
        "%cd '/content/drive/My Drive/JEI pulsar test data'\n",
        "\n",
        "test_data = tf.keras.utils.image_dataset_from_directory('/content/drive/My Drive/JEI pulsar test data',batch_size=1)\n",
        "\n",
        "test_data_iterator = test_data.as_numpy_iterator()\n",
        "test_batch = test_data_iterator.next()\n",
        "\n",
        "test_data = test_data.map(lambda x,y: (x/255, y))\n",
        "test_scaled_iterator = test_data.as_numpy_iterator()\n",
        "test_batch = test_scaled_iterator.next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqyMPGas9vFd"
      },
      "outputs": [],
      "source": [
        "test_size = int(len(test_data))\n",
        "test = test_data.take(test_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi94sxSESn_n",
        "outputId": "4017cb8b-29c0-4061-e6af-f2f13a9c2d04",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Precision: 0.0, Recall: 0.0, Accuracy: 0.5\n"
          ]
        }
      ],
      "source": [
        "# making predcitions and evaluation based on metrics of Precision, Recall, and BinaryAccuracy\n",
        "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy\n",
        "\n",
        "pre = Precision()\n",
        "re = Recall()\n",
        "acc = BinaryAccuracy()\n",
        "\n",
        "for batch in test.as_numpy_iterator():\n",
        "    X, y = batch\n",
        "    yhat = model.predict(X)\n",
        "    pre.update_state(y, yhat)\n",
        "    re.update_state(y, yhat)\n",
        "    acc.update_state(y, yhat)\n",
        "\n",
        "test_precision=float(pre.result().numpy())\n",
        "test_recall=float(re.result().numpy())\n",
        "test_accuracy=float(acc.result().numpy())\n",
        "\n",
        "test_data={'test_accuracy':test_accuracy, 'test_precision':test_precision, 'test_recall':test_recall}\n",
        "print(f'Precision: {test_precision}, Recall: {test_recall}, Accuracy: {test_accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR7rB9v8Suld"
      },
      "source": [
        "# **LOAD DATA INTO CSV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hxKui7iJnz6"
      },
      "outputs": [],
      "source": [
        "#train_data refers to the data stored of metric values from training process\n",
        "\n",
        "train_data = hist.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFT5kQZzLSGM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#removing validation accuracy and loss values from data\n",
        "\n",
        "if 'val_accuracy' in train_data:\n",
        "    del train_data['val_accuracy']\n",
        "\n",
        "if 'val_loss' in train_data:\n",
        "    del train_data['val_loss']\n",
        "\n",
        "print(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBEsxQjDT3pl"
      },
      "source": [
        "**W/ JUST FINAL TRAINING ACCURACY** (this includes only the final training accuracy and loss for all the epochs run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iT2rnkWNO6oa"
      },
      "outputs": [],
      "source": [
        "# getting each last value from training data and put it into a dictionary\n",
        "end_accuracy, end_loss=train_data['accuracy'][-1], train_data['loss'][-1]\n",
        "end_train_data = {'accuracy': end_accuracy, 'loss': end_loss}\n",
        "\n",
        "# make another dict with batch and trial_num for training data with only end training accuracies, add that, and then also test data to end_all_info\n",
        "end_all_info={'batch_size': batch_size, 'trial_num' : trial_num}\n",
        "end_all_info.update(end_train_data)\n",
        "# adding test data to \"end_all_info\" dictionary\n",
        "end_all_info.update(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NzjmxeqoPr6",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Specify the CSV file name\n",
        "end_file_path = '/content/drive/My Drive/BatchSize data.csv'\n",
        "\n",
        "with open(end_file_path, mode='a', newline='') as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=end_all_info.keys())\n",
        "\n",
        "    # Write the header only if the file is empty\n",
        "    if file.tell() == 0:\n",
        "      writer.writeheader()\n",
        "\n",
        "    # Write the dictionary as a row in the CSV file\n",
        "    writer.writerow(end_all_info)\n",
        "\n",
        "print(f\"Data appended to {end_file_path} successfully.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJv05AyVqr3bMFXy7q0XH1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}